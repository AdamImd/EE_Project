\documentclass{article}
\usepackage{style,times}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig,graphicx}
\usepackage{algorithmic}
\usepackage{amsmath}
% For code listings with syntax highlighting
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=none,
    backgroundcolor=\color{gray!10}
}

\title{Conditioning Robotic Action Prediction Using Pretrained Video Generation Models}
\lhead{EE8520 - F25 - Project Intermediate Report}
\rhead{\today}

\author{Adam Imdieke\\
Video Embodiment\\
\texttt{imdie022}@umn.edu}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\finalcopy
\begin{document}

\maketitle

\begin{abstract}
    We propose a novel, zero-shot approach to robotic manipulation that leverages pretrained video generation models to predict intermediate and goal states, for conditioning a low-level action prediction model. We demonstrate the effectiveness of our method on a variety of manipulation tasks, showing performance on tasks never seen during training.
\end{abstract}

\section{Introduction}
Robotics has witnessed significant advancements in recent years due to the integration of generative AI techniques. This is evident in the large parameter foundation models that have revolutionized fields like computer vision and natural language processing. These models utilize internet scale data and immense computational resources to achieve unprecedented levels of performance and generalization.

Despite these advancements, the robotics community still faces challenges in effectively using these models for real-world robotic control and decision-making tasks, especially in dynamic and unstructured environments. The specific knowledge required to understand 3D space and the physical interactions of the robot with its environment is referred to as embodied knowledge. Embodied knowledge varies between robots and environments, making it difficult use  generalized models as they lack this crucial context.
Another downside of these large models is that they often require significant computational resources and computational time, which can be prohibitive for real-time robotic applications. 

We propose a novel approach that leverages pretrained video generation models to condition robotic action prediction. By utilizing the large scale datasets and powerful prediction capabilities of these models, we can predict intermediate and goal images that guide the robot's actions, enabling our lower level action prediction model to use a better aligned goal representation than text. In this work, we use the term ``policy'' to mean a learned model that maps past observations and actions to the next action the robot should take.

\section{Related Works}
Recent works have explored the use of video prediction for robotic control, directly extracting poses or trajectory information from predicted future frames~\citep{Li2025NovaFlowZM,patel2025roboticmanipulationimitatinggenerated}, by predicting depth frames from each predicted frame, providing a dense 3D representation of the scene.

\citet{black2023zero} propose a method that uses a pretrained image-editing diffusion model to perform zero-shot robotic manipulation. They create a goal image from a text description and a current image using the diffusion model, and then train a goal conditioned behavior cloning (GCBC) model to imitate the predicted actions. We aim to build upon this by integrating video prediction to generate more informative goal and sub-goal representations.

\citet{patel2025roboticmanipulationimitatinggenerated} proposes a method that leverages generated videos and predicted depth images to predict the motion of an object in the scene, enabling the robot to imitate the predicted motion without requiring physical demonstrations. This has similar limitations to \citet{Li2025NovaFlowZM} where the object must start grasped, and the object should be large to ensure a quality prediction of the pose. 

\citet{Li2025NovaFlowZM} proposed a zero-shot manipulation method that predicts a flow of keypoints on the object to be manipulated, enabling the robot to infer the necessary actions to achieve the desired state. This approach leverages the generalization of large-scale video prediction models to understand object dynamics, and uses the motion to create a trajectory for the robot to follow. This approach requires the object to be grasped at the start of the interaction, and does not address the full capabilities of the robot's embodiment. The predicted flow is also generated once and cannot allow for a lower level policy to react to new observations or changes in the environment.

Diffusion Policy \citet{chi2023diffusion} is a recent manipulation method that formulates policy learning as a conditional diffusion process, enabling complex, long-horizon robotic tasks to be modeled effectively. It provides state-of-the-art performance on a variety of challenging manipulation tasks, but is limited in generalization by the lack of widespread labeled robotic manipulation data. We aim to address this limitation by defining tasks implicitly using visual goal states, allowing for unlabeled data to be used for training.

We will build upon this line of work by using the same video prediction model \citet{wan2025} as \citet{Li2025NovaFlowZM} to predict a sequence of future frames to be used as goal conditioning for a lower level Diffusion Policy \citet{chi2023diffusion}. Diffusion policy is usually condition on the current observation and a text embedding representing the task, but we propose to replace the text embedding with the predicted future frames, providing a better alignment between the embeddings for the observations and goals. This approach also reduces the need for the high frequency diffusion policy to explicitly model the entire task, focusing instead on short-term goal achievement.
    

\section{Methodology}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Figure.pdf}
    \caption{Overview of our approach: We use a pretrained video generation model to predict a sequence of future frames conditioned on the current observation and a high-level task description. These predicted frames serve as a goal for a low-level policy that executes actions to achieve the desired outcome.}
    \label{fig:overview}
\end{figure}

We propose a hierarchical approach that integrates video prediction with diffusion-based policy learning. We take advantage of the complementary strengths of both paradigms to address the challenges of long-horizon tasks, with the reactive lower-level policy handling the complexities of real-time control.

Most existing robotics models do not explicitly indicate when a task is complete, often relying on a fixed time limit or a set number of actions. To address this, I will consider including a time embedding to help the model understand the sequence and timing of goals. This work represents a new approach to conditioning diffusion policies, rather than an implementation trick, and is distinct from my prior research, which has focused on hardware design and sensing rather than generative models.

Our project is centered on the generative action model rather than on training a new video generator. The pretrained video diffusion model is treated as a high-level planner that provides future visual context, while the main contribution of this work is a diffusion-based action generation model. This model operates over a one-dimensional sequence of action vectors (the robot trajectory) and is conditioned on predicted future images, analogous to how language models predict the next word given a sentence. This framing places the core of the work  within generative modeling for control, which aligns with the scope of EE8520.

\subsection{High-Level Video Prediction}

We utilize the video prediction model from \citet{wan2025} to generate our sequence of future frames, which will act as the high-level plan for the robot to follow. The video prediction model $f_{\phi}^{\mathrm{video}}$ takes as input the current visual observation $o_t$ and a text command $c$, outputting a sequence of predicted future observations $\hat{o}_{t+1:t+H} = \{\hat{o}_{t+1}, \hat{o}_{t+2}, ..., \hat{o}_{t+H}\}$, where $H$ is the prediction horizon. An example of the predicted frames is shown in Figure~\ref{fig:overview} for the text prompt: "Pick up the red block".

\subsection{Low-Level Policy with Diffusion}

Using the predicted future observations at time step $t$, we define our sequence of visual goals $g_{t+1:t+H} = \{g_{t+1}, g_{t+2}, ..., g_{t+H}\}$, where each $g_{t+k}$ is derived from $\hat{o}_{t+k}$ and serves as a goal state for control. The low-level, goal-conditioned policy is denoted by $\pi_\theta(a_{t:t+K} \mid o_t, g_t)$, which generates a sequence of actions $a_{t:t+K} = \{a_t, a_{t+1}, ..., a_{t+K}\}$ that moves the robot towards the current goal representation $g_t$, where $K$ is the action horizon. The low-level actions occur at a much higher frequency than the video prediction, i.e., the control step is much smaller than the video step.

We consider several ways of constructing the goal representation $g_t$ from the predicted sequence $g_{t+1:t+H}$, including using the final predicted frame $g_t = g_{t+H}$, using a single intermediate frame $g_t = g_{t+k}$ where $1 < k < H$, or computing an attention-based goal $g_t = \mathrm{Attn}(o_t, g_{t+1:t+H})$ that selects or blends information from the entire predicted sequence. For all approaches we use a common ResNet encoder to embed both observations and goals into a shared representation space.

Following Diffusion Policy \citep{chi2023diffusion}, we model $\pi_\theta(a_{t:t+K} \mid o_t, g_t)$ as a conditional diffusion process over the action sequence. The diffusion model starts from a sequence of Gaussian noise and iteratively denoises it conditioned on the current observation and goal embeddings, gradually refining the action sequence to produce a coherent plan that makes progress towards the specified goal.

Our approach is novel in that, while prior works use predicted video frames to create goals for denoising actions or directly extract robot poses from video, these methods often lack the ability to react to changes in the environment or rely on single goal images. In contrast, we create a series of goal images from a predicted video and condition the action diffusion model on these multiple future frames. This enables richer, temporally-aware guidance for action generation. We hypothesize that conditioning diffusion on future goal images will outperform conditioning on text embeddings of the task description. To further improve flexibility, we will explore using attention mechanisms to weight which goal image is most relevant at each step. To our knowledge, conditioning diffusion using multiple future goal images in this way has not been previously explored.

\subsection{Training}

We use the pretrained video model without fine-tuning, as it is extremely computationally intensive to train. We produce one video per command, with an average generation time over 20 minutes on a single NVIDIA A5500 GPU. Because of the high computational cost, we only generate videos during rollout, instead training the diffusion policy on offline data.

To collect data for the Diffusion Policy, we teleoperate the robot to perform exploratory actions. There is no need to use labeled data, as the diffusion model can learn from the raw state-action trajectories in an autoregressive manner. For this reason, we do not require expensive annotation or task-specific training, preserving the generality of our approach.

We aim to collect a dataset of 2 hours of robot teleoperation data in the tabletop setting with diverse objects and configurations, capturing a wide range of manipulation behaviors.

In our approach, task specification and planning are effectively offloaded to the video model, which generates future goal images for the robot to pursue. While the training data is described as "unlabeled," it is analogous to NLP datasets, where the model learns from sequences of actions chosen by a human, without explicit class or label annotations. The diffusion model is trained on these raw state-action trajectories, and we aim to demonstrate that goal-conditioned planning can be effective even with limited data. Evaluation will focus on the model's ability to generalize to new tasks and objects it has not encountered during training.

Our dataset $\mathcal{D} = \{(o_{1:T^{(i)}}^{(i)}, a_{1:T^{(i)}}^{(i)})\}_{i=1}^N$ consists of full trajectories of observations and actions, where $T^{(i)}$ is the length of trajectory $i$. From this dataset, the model learns to predict the next $K$ actions given the current observation and goal embedding.


\section{Updates}

\subsection{Computation devices}
To execute my Diffusion Policy code, I will use my personal worksatation that has two NVIDIA A5500 GPUs with 24GB of VRAM each. 
To render the videos, I will use my lab's server that has 8 NVIDIA A6000 with 49GB of VRAM each.

\subsection{Current Status}
The theoretical part of this project has been implemented in PyTorch so far. I am using the offical Diffusion Policy repository as a starting place. So far I have:
\begin{list}{*}{ }
    \item Implemented Video prediction using the WAN model from \citet{wan2025}. The outputs can be seen in Figure~\ref{fig:overview}.
    \item Implemented the attention mechanism to condition the diffusion policy on multiple future goal images.
    \item Started work on the data collection pipeline using our robot setup. 
\end{list}

We have these items left to complete before the end of the semester:
\begin{list}{*}{ }
    \item Complete data collection using our robot setup.
    \item Train the diffusion policy using the collected data and evaluate on a set of manipulation tasks
    \item Improve prompting or image viewpoints to improve video prediction quality.
    \item Evaluate the effectiveness of this method.
\end{list}

\subsection{Video Prediction Results}
\begin{figure}
    \centering
    \begin{tabular}{ccccc}
        \includegraphics[width=0.18\linewidth]{./out/Good_Untie_figs/frame_0000.png} &
        \includegraphics[width=0.18\linewidth]{./out/Good_Untie_figs/frame_0001.png} &
        \includegraphics[width=0.18\linewidth]{./out/Good_Untie_figs/frame_0002.png} &
        \includegraphics[width=0.18\linewidth]{./out/Good_Untie_figs/frame_0003.png} &
        \includegraphics[width=0.18\linewidth]{./out/Good_Untie_figs/frame_0004.png} \\
        \includegraphics[width=0.18\linewidth]{./out/480_untie_bad/frame_0000.png} &
        \includegraphics[width=0.18\linewidth]{./out/480_untie_bad/frame_0001.png} &
        \includegraphics[width=0.18\linewidth]{./out/480_untie_bad/frame_0002.png} &
        \includegraphics[width=0.18\linewidth]{./out/480_untie_bad/frame_0003.png} &
        \includegraphics[width=0.18\linewidth]{./out/480_untie_bad/frame_0004.png}\\
        \includegraphics[width=0.18\linewidth]{./out/480_untie/frame_0000.png} &
        \includegraphics[width=0.18\linewidth]{./out/480_untie/frame_0001.png} &
        \includegraphics[width=0.18\linewidth]{./out/480_untie/frame_0002.png} &
        \includegraphics[width=0.18\linewidth]{./out/480_untie/frame_0003.png} &
        \includegraphics[width=0.18\linewidth]{./out/480_untie/frame_0004.png} \\
        \includegraphics[width=0.18\linewidth]{./out/odd/frame_0000.png} &
        \includegraphics[width=0.18\linewidth]{./out/odd/frame_0001.png} &
        \includegraphics[width=0.18\linewidth]{./out/odd/frame_0002.png} &
        \includegraphics[width=0.18\linewidth]{./out/odd/frame_0003.png} &
        \includegraphics[width=0.18\linewidth]{./out/odd/frame_0004.png} \\
        \includegraphics[width=0.18\linewidth]{./out/720_figs/frame_0000.png} &
        \includegraphics[width=0.18\linewidth]{./out/720_figs/frame_0001.png} &
        \includegraphics[width=0.18\linewidth]{./out/720_figs/frame_0002.png} &
        \includegraphics[width=0.18\linewidth]{./out/720_figs/frame_0003.png} &
        \includegraphics[width=0.18\linewidth]{./out/720_figs/frame_0004.png} \\
    \end{tabular}
    \caption{Video prediction results showing generated frames given the starting image. \\
    \textbf{Rows 1-3:} Prompt: "Untie the knot". \\
    \textbf{Rows 4-5:} Prompt: "The robot puts the banana into the bowl". \\
    \textbf{Row 1:} Best performing prediction using 720p 14B model (80 min generation time). \\
    \textbf{Row 2:} Poor performance with gripper artifacts using 480p 14B model (65 min). \\
    \textbf{Row 3:} 720p model at 480p resolution, improved but suboptimal (65 min). \\
    \textbf{Row 4:} Robot not well understood, 720p model, 11 sec duration (110 min). \\
    \textbf{Row 5:} Human-centric interpretation, 720p model, 5 sec duration (75 min).\\
    Check out this link to see the videos: \href{https://www.notion.so/Generated-Videos-2aec5a4440b780209657c6bbdf18d5a6?source=copy_link}{LINK}}
    \label{fig:video_results}
\end{figure}

From figure \ref{fig:video_results} we can see that the video prediction model may be able to generate plausible future frames sometimes, but it is very expensive to run, and the quality is inconsistent. Further work is needed to improve the promptinng and choose a better camera viewpoint to ensure the robot and objects are clearly visible and move in a physically plausible manner. 

While we may be able to generate some reasonable videos, it is unclear if this will be sufficent to test our hypothesis that conditioning on predicted images is better than text. To effedctively evaluate this, I will devise a pipeline to allow the operator to do a task, and then replay the acitons in reverse to reset the scene to the original state. This will allow for the future video of the scene to be generated by humans, and then the robot can attempt to imitate the human demonstrated task using the human generated video as the goal.

Given this project is not focused on video prediction, I will not spend excessive time trying to improve the video generation quality, but rather focus on getting a reasonable baseline working with the current model.


\subsection{Diffusion Policy Conditioning}

The proposed algorithm is summarized in Figure~\ref{fig:algorithm}, where I outline the two main steps: First, the video prediction model generates future frames based on the current observation and text command. These frames are then encoded to form goal representations. Second, the diffusion policy uses these goal representations, along with the current observation and proprioception, to denoise the initial noise action sequence into an action plan for the robot. The cross-attention mechanism allows the policy to effectively incorporate information from multiple predicted goal images, allowing it to translate from future visual goals to robot actions. 
\begin{figure}[h]
    \centering
    \begin{minipage}{0.95\linewidth}
        \begin{algorithmic}[1]
            \STATE \textbf{Input:} Current observation $o_t$, text command $c$, proprioception $p_t$
            \STATE \textbf{Output:} Action sequence $a_{t:t+K}$
            
            \STATE \textcolor{blue}{\textbf{// Step 1: Video Prediction (High-Level Planning: Slow)}}
            \STATE $\hat{o}_{t+1:t+H} \leftarrow f_{\phi}^{\mathrm{video}}(o_t, c)$ \COMMENT{Generate future frames}
            \STATE $g_{t+1:t+H} \leftarrow \text{Encode}(\hat{o}_{t+1:t+H})$ \COMMENT{Encode goal images}
            
            \STATE \textcolor{red}{\textbf{// Step 2: Diffusion Policy with Cross-Attention Conditioning}}
            \STATE \textcolor{red}{\textbf{// Step 2: Action Generation (Low-Level Control: Fast)}}
            \WHILE{Task not done}
                \STATE $a^{(0)}_{t:t+K} \sim \mathcal{N}(0, I)$ \COMMENT{Initialize action sequence as noise}
                \STATE $o_t^{\text{enc}} \leftarrow \text{Encode}(o_t)$ \COMMENT{Encode current observation}
                \STATE $p_t^{\text{enc}} \leftarrow \text{MLP}(p_t)$ \COMMENT{Encode proprioception}
                
                \FOR{diffusion step $\tau = T, T-1, \ldots, 1$}
                    \STATE \textcolor{purple}{\textbf{// Cross-Attention over Generated Images}}
                    \STATE $\text{attn\_out} \leftarrow \text{CrossAttention}($ 
                    \STATE \quad \quad $\text{query}=o_t^{\text{enc}}, \text{key}=g_{t+1:t+H}, \text{value}=g_{t+1:t+H})$
                    
                    \STATE \textcolor{purple}{\textbf{// Construct Conditioning Vector}}
                    \STATE $\text{cond} \leftarrow \text{Concat}(o_t^{\text{enc}}, \text{attn\_out}, p_t^{\text{enc}})$
                    
                    \STATE \textcolor{purple}{\textbf{// 1D Convolution over Action Horizon}}
                    \STATE $\epsilon_\theta \leftarrow \text{ConditionalUNet1D}(a^{(\tau)}_{t:t+K}, \tau, \text{cond})$
                    
                    \STATE \textcolor{purple}{\textbf{// Denoising Step}}
                    \STATE $a^{(\tau-1)}_{t:t+K} \leftarrow \text{Scheduler.step}(\epsilon_\theta, \tau, a^{(\tau)}_{t:t+K})$
                \ENDFOR
                
                \STATE Execute $a_t$ and update $t \leftarrow t+1$
                \STATE Observe new $o_t$ and $p_t$
            \ENDWHILE

            \STATE \textbf{return} $a^{(0)}_{t:t+K}$ \COMMENT{Final denoised action sequence}
        \end{algorithmic}
    \end{minipage}
    \caption{Two-step algorithm: (Step 1) Pre-trained video prediction generates future goal images, (Step 2) Diffusion policy uses cross-attention to condition on these images, transforming noise into coherent action sequences through iterative denoising over the action horizon using 1D convolutions.}
    \label{fig:algorithm}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}{0.8\linewidth}
        \begin{lstlisting}
def forward(self, obs_dict, future_frames=None):
    """Encode current obs, attend to future frames, append attention output."""
    # Encode current observation using parent class
    current_obs_features = super().forward(obs_dict)  # (B, D)
    
    if future_frames is not None:
        B, H, C, img_H, img_W = future_frames.shape
        future_features = []
        primary_rgb_key = self.rgb_keys[0] if self.rgb_keys else None
        for h in range(H):
            future_obs_dict = {}
            if primary_rgb_key:
                future_obs_dict[primary_rgb_key] = future_frames[:, h]
            frame_feature = super().forward(future_obs_dict)  # (B, D)
            future_features.append(frame_feature)
        future_features = torch.stack(future_features, dim=1)  # (B, H, D)
        
        # Project to attention dimensions
        current_projected = self.obs_proj(current_obs_features).unsqueeze(1)
        future_projected = self.future_proj(future_features)

        # Cross-attention: Query from current, Key/Value from future
        attn_out, _ = self.cross_attention(
            query=current_projected,
            key=future_projected,
            value=future_projected
        )
        attn_out = self.output_proj(attn_out.squeeze(1))  # (B, obs_dim)
        result = torch.cat([current_obs_features, attn_out], dim=-1)
    else:
        result = current_obs_features
    return result
        \end{lstlisting}
    \end{minipage}
    \caption{Code for \texttt{MultiImageObsEncoder\_Video.forward}: encodes current observation with \texttt{super().forward}, encodes future frames, computes cross-attention (current as query, futures as key/value), and concatenates the attention output to the original features for conditioning.}
    \label{fig:code}
\end{figure}

The core modification is in the observation encoder, where we append the attention output to the original observation features. The hope is that the future images are in the same modality as the observations, allowing for better alignment compared to text embeddings. The code for this encoder is shown in Figure~\ref{fig:code}. (This is at the bottom of the document)
    

\bibliography{refs}
\bibliographystyle{plainnat}

\end{document}