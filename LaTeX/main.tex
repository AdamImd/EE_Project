\documentclass{article}
\usepackage{style,times}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig,graphicx}

\title{Conditioning Robotic Action Prediction Using Pretrained Video Generation Models}
\lhead{EE8520 - F25 - Project Proposal}
\rhead{\today}

\author{Adam Imdieke\\
Video Embodiment\\
\texttt{imdie022}@umn.edu}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\finalcopy
\begin{document}

\maketitle

\begin{abstract}
    We propose a novel, zero-shot approach to robotic manipulation that leverages pretrained video generation models to predict intermediate and goal states, for conditioning a low-level action prediction model. We demonstrate the effectiveness of our method on a variety of manipulation tasks, showing performance on tasks never seen during training.
\end{abstract}

\section{Introduction}
Robotics has witnessed significant advancements in recent years due to the integration of generative AI techniques. This is evident in the large parameter foundation models that have revolutionized fields like computer vision and natural language processing. These models utilize internet scale data and immense computational resources to achieve unprecedented levels of performance and generalization.

Despite these advancements, the robotics community still faces challenges in effectively using these models for real-world robotic control and decision-making tasks, especially in dynamic and unstructured environments. The specific knowledge required to understand 3D space and the physical interactions of the robot with its environment is referred to as embodied knowledge. Embodied knowledge varies between robots and environments, making it difficult use  generalized models as they lack this crucial context.
Another downside of these large models is that they often require significant computational resources and computational time, which can be prohibitive for real-time robotic applications. 

We propose a novel approach that leverages pretrained video generation models to condition robotic action prediction. By utilizing the large scale datasets and powerful prediction capabilities of these models, we can predict intermediate and goal images that guide the robot's actions, enabling our lower level action prediction model to use a better aligned goal representation than text. In this work, we use the term ``policy'' to mean a learned model that maps past observations and actions to the next action the robot should take.

\section{Related Works}
Recent works have explored the use of video prediction for robotic control, directly extracting poses or trajectory information from predicted future frames~\citep{Li2025NovaFlowZM,patel2025roboticmanipulationimitatinggenerated}, by predicting depth frames from each predicted frame, providing a dense 3D representation of the scene.

\citet{black2023zero} propose a method that uses a pretrained image-editing diffusion model to perform zero-shot robotic manipulation. They create a goal image from a text description and a current image using the diffusion model, and then train a goal conditioned behavior cloning (GCBC) model to imitate the predicted actions. We aim to build upon this by integrating video prediction to generate more informative goal and sub-goal representations.

\citet{patel2025roboticmanipulationimitatinggenerated} proposes a method that leverages generated videos and predicted depth images to predict the motion of an object in the scene, enabling the robot to imitate the predicted motion without requiring physical demonstrations. This has similar limitations to \citet{Li2025NovaFlowZM} where the object must start grasped, and the object should be large to ensure a quality prediction of the pose. 

\citet{Li2025NovaFlowZM} proposed a zero-shot manipulation method that predicts a flow of keypoints on the object to be manipulated, enabling the robot to infer the necessary actions to achieve the desired state. This approach leverages the generalization of large-scale video prediction models to understand object dynamics, and uses the motion to create a trajectory for the robot to follow. This approach requires the object to be grasped at the start of the interaction, and does not address the full capabilities of the robot's embodiment. The predicted flow is also generated once and cannot allow for a lower level policy to react to new observations or changes in the environment.

Diffusion Policy \citet{chi2023diffusion} is a recent manipulation method that formulates policy learning as a conditional diffusion process, enabling complex, long-horizon robotic tasks to be modeled effectively. It provides state-of-the-art performance on a variety of challenging manipulation tasks, but is limited in generalization by the lack of widespread labeled robotic manipulation data. We aim to address this limitation by defining tasks implicitly using visual goal states, allowing for unlabeled data to be used for training.

We will build upon this line of work by using the same video prediction model \citet{wan2025} as \citet{Li2025NovaFlowZM} to predict a sequence of future frames to be used as goal conditioning for a lower level Diffusion Policy \citet{chi2023diffusion}. Diffusion policy is usually condition on the current observation and a text embedding representing the task, but we propose to replace the text embedding with the predicted future frames, providing a better alignment between the embeddings for the observations and goals. This approach also reduces the need for the high frequency diffusion policy to explicitly model the entire task, focusing instead on short-term goal achievement.
    

\section{Methodology}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Figure.pdf}
    \caption{Overview of our approach: We use a pretrained video generation model to predict a sequence of future frames conditioned on the current observation and a high-level task description. These predicted frames serve as a goal for a low-level policy that executes actions to achieve the desired outcome.}
    \label{fig:overview}
\end{figure}

We propose a hierarchical approach that integrates video prediction with diffusion-based policy learning. We take advantage of the complementary strengths of both paradigms to address the challenges of long-horizon tasks, with the reactive lower-level policy handling the complexities of real-time control.

Most existing robotics models do not explicitly indicate when a task is complete, often relying on a fixed time limit or a set number of actions. To address this, I will consider including a time embedding to help the model understand the sequence and timing of goals. This work represents a new approach to conditioning diffusion policies, rather than an implementation trick, and is distinct from my prior research, which has focused on hardware design and sensing rather than generative models.

Our project is centered on the generative action model rather than on training a new video generator. The pretrained video diffusion model is treated as a high-level planner that provides future visual context, while the main contribution of this work is a diffusion-based action generation model. This model operates over a one-dimensional sequence of action vectors (the robot trajectory) and is conditioned on predicted future images, analogous to how language models predict the next word given a sentence. This framing places the core of the work  within generative modeling for control, which aligns with the scope of EE8520.

\subsection{High-Level Video Prediction}

We utilize the video prediction model from \citet{wan2025} to generate our sequence of future frames, which will act as the high-level plan for the robot to follow. The video prediction model $f_{\phi}^{\mathrm{video}}$ takes as input the current visual observation $o_t$ and a text command $c$, outputting a sequence of predicted future observations $\hat{o}_{t+1:t+H} = \{\hat{o}_{t+1}, \hat{o}_{t+2}, ..., \hat{o}_{t+H}\}$, where $H$ is the prediction horizon. An example of the predicted frames is shown in Figure~\ref{fig:overview} for the text prompt: "Pick up the red block".

\subsection{Low-Level Policy with Diffusion}

Using the predicted future observations at time step $t$, we define our sequence of visual goals $g_{t+1:t+H} = \{g_{t+1}, g_{t+2}, ..., g_{t+H}\}$, where each $g_{t+k}$ is derived from $\hat{o}_{t+k}$ and serves as a goal state for control. The low-level, goal-conditioned policy is denoted by $\pi_\theta(a_{t:t+K} \mid o_t, g_t)$, which generates a sequence of actions $a_{t:t+K} = \{a_t, a_{t+1}, ..., a_{t+K}\}$ that moves the robot towards the current goal representation $g_t$, where $K$ is the action horizon. The low-level actions occur at a much higher frequency than the video prediction, i.e., the control step is much smaller than the video step.

We consider several ways of constructing the goal representation $g_t$ from the predicted sequence $g_{t+1:t+H}$, including using the final predicted frame $g_t = g_{t+H}$, using a single intermediate frame $g_t = g_{t+k}$ where $1 < k < H$, or computing an attention-based goal $g_t = \mathrm{Attn}(o_t, g_{t+1:t+H})$ that selects or blends information from the entire predicted sequence. For all approaches we use a common ResNet encoder to embed both observations and goals into a shared representation space.

Following Diffusion Policy \citep{chi2023diffusion}, we model $\pi_\theta(a_{t:t+K} \mid o_t, g_t)$ as a conditional diffusion process over the action sequence. The diffusion model starts from a sequence of Gaussian noise and iteratively denoises it conditioned on the current observation and goal embeddings, gradually refining the action sequence to produce a coherent plan that makes progress towards the specified goal.

Our approach is novel in that, while prior works use predicted video frames to create goals for denoising actions or directly extract robot poses from video, these methods often lack the ability to react to changes in the environment or rely on single goal images. In contrast, we create a series of goal images from a predicted video and condition the action diffusion model on these multiple future frames. This enables richer, temporally-aware guidance for action generation. We hypothesize that conditioning diffusion on future goal images will outperform conditioning on text embeddings of the task description. To further improve flexibility, we will explore using attention mechanisms to weight which goal image is most relevant at each step. To our knowledge, conditioning diffusion using multiple future goal images in this way has not been previously explored.

\subsection{Training}

We use the pretrained video model without fine-tuning, as it is extremely computationally intensive to train. We produce one video per command, with an average generation time over 20 minutes on a single NVIDIA A5500 GPU. Because of the high computational cost, we only generate videos during rollout, instead training the diffusion policy on offline data.

To collect data for the Diffusion Policy, we teleoperate the robot to perform exploratory actions. There is no need to use labeled data, as the diffusion model can learn from the raw state-action trajectories in an autoregressive manner. For this reason, we do not require expensive annotation or task-specific training, preserving the generality of our approach.

We aim to collect a dataset of 2 hours of robot teleoperation data in the tabletop setting with diverse objects and configurations, capturing a wide range of manipulation behaviors.

In our approach, task specification and planning are effectively offloaded to the video model, which generates future goal images for the robot to pursue. While the training data is described as "unlabeled," it is analogous to NLP datasets, where the model learns from sequences of actions chosen by a human, without explicit class or label annotations. The diffusion model is trained on these raw state-action trajectories, and we aim to demonstrate that goal-conditioned planning can be effective even with limited data. Evaluation will focus on the model's ability to generalize to new tasks and objects it has not encountered during training.

Our dataset $\mathcal{D} = \{(o_{1:T^{(i)}}^{(i)}, a_{1:T^{(i)}}^{(i)})\}_{i=1}^N$ consists of full trajectories of observations and actions, where $T^{(i)}$ is the length of trajectory $i$. From this dataset, the model learns to predict the next $K$ actions given the current observation and goal embedding.

\bibliography{refs}
\bibliographystyle{refs}    

\end{document}